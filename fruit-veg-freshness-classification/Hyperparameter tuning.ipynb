{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import ResNet50 pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers:  175\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "img_size = (img_height, img_width)\n",
    "img_shape = img_size + (3,)\n",
    "\n",
    "pre_trained_model = ResNet50(input_shape = img_shape,\n",
    "                        include_top = False,\n",
    "                        weights = 'imagenet')\n",
    "\n",
    "print(\"Number of layers: \", len(pre_trained_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4136 files belonging to 18 classes.\n",
      "Using 3309 files for training.\n",
      "Found 4136 files belonging to 18 classes.\n",
      "Using 827 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Edit according to local path for dataset\n",
    "ds_path = r\"fruitveg\"\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(ds_path,\n",
    "                                        validation_split = 0.2,\n",
    "                                        subset = \"training\",\n",
    "                                        seed = 123,\n",
    "                                        image_size = img_size,\n",
    "                                        batch_size = batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(ds_path,\n",
    "                                      validation_split = 0.2,\n",
    "                                      subset = \"validation\",\n",
    "                                      seed = 123,\n",
    "                                      image_size = img_size,\n",
    "                                      batch_size = batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(1e-5, 1e-1))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.4))\n",
    "HP_TUNING_LAYER = hp.HParam('tuning_layers', hp.IntInterval(150, 175))\n",
    "# HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LEARNING_RATE, HP_DROPOUT, HP_TUNING_LAYER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training & run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(hparams):\n",
    "    pre_trained_model.trainable = True\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    START_TRAIN = hparams[HP_TUNING_LAYER]\n",
    "\n",
    "    # Freeze all the layers before \n",
    "    for layer in pre_trained_model.layers[:START_TRAIN]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Averaging layer\n",
    "    global_average = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "    # Data augmentation \n",
    "    augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "        tf.keras.layers.RandomRotation(0.2)\n",
    "    ])\n",
    "\n",
    "    # Add dense layer\n",
    "    prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "\n",
    "    # Chain model \n",
    "    inputs = tf.keras.Input(shape = img_shape)\n",
    "    x = augmentation(inputs) \n",
    "    x = preprocess_input(x)\n",
    "    x = pre_trained_model(x, training=False)\n",
    "    x = global_average(x)\n",
    "    x = tf.keras.layers.Dropout(hparams[HP_DROPOUT])(x)\n",
    "    outputs = prediction_layer(x)\n",
    "    model = tf.keras.Model(inputs,outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    LR = hparams[HP_LEARNING_RATE]\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits = False),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    EPOCH = 5\n",
    "\n",
    "    model.fit(train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs = EPOCH,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(logdir),  \n",
    "            hp.KerasCallback(logdir, hparams)\n",
    "            ]\n",
    "        )\n",
    "    _, accuracy = model.evaluate(train_ds, val_ds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = model_training(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'learning_rate': 1e-05, 'dropout': 0.1, 'tuning_layers': 150}\n",
      "Epoch 1/5\n",
      " 85/104 [=======================>......] - ETA: 1:05 - loss: 2.5237 - accuracy: 0.2279"
     ]
    }
   ],
   "source": [
    "session_num = 0\n",
    "logdir = r'logs/hparam_tuning'\n",
    "\n",
    "for learning_rate in (HP_LEARNING_RATE.domain.min_value,\n",
    "                        HP_LEARNING_RATE.domain.max_value):\n",
    "    for dropout_rate in (HP_DROPOUT.domain.min_value,\n",
    "                         HP_DROPOUT.domain.max_value):\n",
    "        for layer in (HP_TUNING_LAYER.domain.min_value,\n",
    "                             HP_TUNING_LAYER.domain.max_value):\n",
    "            hparams = {\n",
    "                  HP_LEARNING_RATE: learning_rate,\n",
    "                  HP_DROPOUT: dropout_rate,\n",
    "                  HP_TUNING_LAYER: layer,\n",
    "              }\n",
    "            run_name = \"run-%d\" % session_num\n",
    "            print('--- Starting trial: %s' % run_name)\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            run('logs/hparam_tuning/' + run_name, hparams)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
